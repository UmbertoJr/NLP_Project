{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicate_detection.dataProcessing import read_file, model_builder\n",
    "from importlib import reload \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total numeber of fittable predicates are:  733\n",
      "possible POS have length:  48\n",
      "maximum length of sentence is  144\n",
      "Ms. Haag plays Elianti .\n",
      "['1', 'Ms.', 'ms.', 'ms.', 'NNP', 'NNP', '_', '_', '2', '2', 'TITLE', 'TITLE', '_', '_', '_\\n']\n",
      "['2', 'Haag', 'haag', 'haag', 'NNP', 'NNP', '_', '_', '3', '3', 'SBJ', 'SBJ', '_', '_', 'A0\\n']\n",
      "['3', 'plays', 'play', 'play', 'VBZ', 'VBZ', '_', '_', '0', '0', 'ROOT', 'ROOT', 'Y', 'play.02', '_\\n']\n",
      "['4', 'Elianti', 'elianti', 'elianti', 'NNP', 'NNP', '_', '_', '3', '3', 'OBJ', 'OBJ', '_', '_', 'A1\\n']\n",
      "['5', '.', '.', '.', '.', '.', '_', '_', '3', '3', 'P', 'P', '_', '_', '_\\n']\n"
     ]
    }
   ],
   "source": [
    "file = \"./data/CoNLL2009-ST-English-train.txt\"\n",
    "reader = read_file(file)\n",
    "\n",
    "\n",
    "pred_to_fit, pred_not = reader.find_all_predicates(occ_greater_than=50)  # I fit on my model just the predicate seen more than 50 times\n",
    "print(\"total numeber of fittable predicates are: \", len(pred_to_fit))\n",
    "pos, pos_pos = reader.find_all_POS() # ritorna la lista di possibili POS e il contatore di quanti predicati si trovano in quel POS\n",
    "print(\"possible POS have length: \", len(pos_pos))\n",
    "max_len_in_data = reader.max_length_sentence()\n",
    "print(\"maximum length of sentence is \", max_len_in_data)\n",
    "len_sent = max_len_in_data\n",
    "\n",
    "sents = reader.read_sentences(100) ## just read 10 sentences from the train\n",
    "## vediamo un po' il dataset\n",
    "i = 1\n",
    "reader.print_sentence(sents[i])\n",
    "print(*sents[i], sep=\"\\n\")\n",
    "\n",
    "### il modello serve a creare i dati input per il DNN\n",
    "model = model_builder(pos_pos, max_length = len_sent)\n",
    "list_of_words = model.load_model(\"./data/glove.6B.100d.txt\")\n",
    "#model(\"the\")\n",
    "\n",
    "X_emb, sent_len, pred, X_POS = model.creation(sents, max_length= len_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "install gensim to make word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = reader.read_sentences(1000)\n",
    "X, sent_len, pred, X_POS = model.creation(sents, max_length= len_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## model parameter\n",
    "batch_size = 1000\n",
    "LAYERS = 1\n",
    "hidden_rapr_Bi = 30\n",
    "\n",
    "# Data summary\n",
    "max_length_sentences = len_sent\n",
    "dim_feature = 2\n",
    "\n",
    "# place holders dimentions\n",
    "dimension_embeddings = 100\n",
    "dimension_POS = 48\n",
    "POS_emb_dim = 16\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "g1 = tf.Graph()\n",
    "with g1.as_default():\n",
    "    ## Inputs\n",
    "    one_hot_POS = tf.placeholder(tf.float32, shape = (batch_size, max_length_sentences, dimension_POS), name= \"POS\")\n",
    "    embeddings = tf.placeholder(tf.float32, shape=(batch_size, max_length_sentences , dimension_embeddings), name=\"embeddings\")\n",
    "    sequence_lengths = tf.placeholder(tf.int32, shape = [batch_size])\n",
    "    \n",
    "    ## randomized embeddings for POS\n",
    "    W_emb_pos = tf.get_variable(\"W_emb_pos\", shape = [dimension_POS , POS_emb_dim], dtype = tf.float32)\n",
    "    POS = tf.reshape(one_hot_POS, shape = [-1, dimension_POS ])\n",
    "    POS_emb_flat = POS @ W_emb_pos    \n",
    "    POS_emb = tf.reshape(POS_emb_flat, shape = [ batch_size , max_length_sentences, POS_emb_dim])\n",
    "    \n",
    "    # inputs to the Bi-lstm\n",
    "    inputs = tf.concat([embeddings, POS_emb], axis = -1)\n",
    "\n",
    "    \"\"\"\n",
    "    ## Bi-Lstm for cudnn\n",
    "    bi_lstm = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers = LAYERS, num_units = hidden_rapr_Bi,\n",
    "                                             direction = 'bidirectional',\n",
    "                                             dropout = 0.2,\n",
    "                                             seed = 123, dtype = tf.float32, name=\"Bi-Lstm\")\n",
    "\n",
    "    outputs_Bi_Lstm, output_states = bi_lstm(inputs)\n",
    "\n",
    "    context_flat = tf.reshape(outputs_Bi_Lstm, shape = [-1,2*hidden_rapr_Bi ])\n",
    "    \"\"\"\n",
    "\n",
    "    ## Bi-lstm two layers for cpu \n",
    "    fw_cell_list = [tf.nn.rnn_cell.LSTMCell(hidden_rapr_Bi*i) for i in range(2,0,-1)]\n",
    "    bw_cell_list = [tf.nn.rnn_cell.LSTMCell(hidden_rapr_Bi*i) for i in range(2,0,-1)]\n",
    "    fw_cells = tf.nn.rnn_cell.MultiRNNCell(fw_cell_list)\n",
    "    bw_cells = tf.nn.rnn_cell.MultiRNNCell(bw_cell_list)\n",
    "    outputs , outputs_states = tf.nn.bidirectional_dynamic_rnn(fw_cells, bw_cells, \n",
    "                                                               inputs,\n",
    "                                                               sequence_length=sequence_lengths, dtype= tf.float32)\n",
    "    \n",
    "    output = tf.concat(outputs, 2)\n",
    "    context_flat = tf.reshape(output, shape = [-1,2*hidden_rapr_Bi ])\n",
    "    \n",
    "    ## Last layer weigths\n",
    "    W_out = tf.get_variable(\"W_out\", shape = [2*hidden_rapr_Bi , dim_feature], dtype = tf.float32)\n",
    "    b_out = tf.get_variable(\"b_out\", shape = [dim_feature], dtype = tf.float32, initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "    pred_flat = tf.matmul(context_flat, W_out) + b_out\n",
    "    pred = tf.reshape(pred_flat, shape= [batch_size, max_length_sentences, dim_feature])\n",
    "\n",
    "\n",
    "    ###### Losses and Optimizer\n",
    "    labels = tf.placeholder(tf.int32, shape=[batch_size, max_length_sentences, dim_feature], name= \"labels\")\n",
    "\n",
    "    losses = tf.nn.softmax_cross_entropy_with_logits_v2(logits = pred, labels=labels)\n",
    "\n",
    "    #mask\n",
    "    \n",
    "    mask = tf.sequence_mask(sequence_lengths, max_length_sentences)\n",
    "    losses = tf.boolean_mask(losses, mask)\n",
    "    loss = tf.reduce_mean(losses)\n",
    "\n",
    "    ### Optimizer\n",
    "    lr = 0.01\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    ### labels pred\n",
    "    labels_pred = tf.cast(tf.argmax(pred, axis=-1), dtype= tf.int32, name=\"pred_labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss function:  0.66994506\n",
      "execution time:  14.034213304519653\n",
      "loss function:  0.707205\n",
      "execution time:  562.0122532844543\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "sents = reader.read_sentences(1000)\n",
    "X_emb, sent_len, pred, X_POS = model.creation(sents, max_length= len_sent)\n",
    "\n",
    "feed_dict={one_hot_POS: X_POS[:batch_size],\n",
    "           embeddings: X_emb[:batch_size],\n",
    "           sequence_lengths:sent_len[:batch_size],\n",
    "           labels: pred[:batch_size]\n",
    "          }\n",
    "\n",
    "with g1.as_default():\n",
    "    for ite in range(10000):\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as s:\n",
    "            s.run(tf.initializers.global_variables())\n",
    "            l , _ = s.run([loss, train_op], feed_dict=feed_dict)\n",
    "            if ite % 100==1:\n",
    "                print(\"loss function: \", l)\n",
    "                print(\"execution time: \", time()-start)\n",
    "            \n",
    "print(\"execution time: \", time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
